/*
 * Kneron Model API Manager
 *
 * Copyright (C) 2019 Kneron, Inc. All rights reserved.
 *
 */

#include <string.h>
#include <stdlib.h>
#include "base.h"
#include "kmdw_ipc.h"
#include "kmdw_model.h"
#include "kmdw_console.h"   /*for dbg_msg */

#include "kmdw_memxfer.h"
#include "project.h"        /*for FLASH_MODEL_XXX definition */
#include "kmdw_memory.h"
#include "scu_ipc.h"        /*for NCPU triggering */
#include "kmdw_utils_crc.h"
#include "kdev_flash.h"
#include "kdrv_clock.h"         /* for kdrv_delay_us() */
#include "kdev_sensor.h"


#include "kmdw_stereo_depth.h"
#include "stereo_depth_init.h"
#include "kmdw_dfs.h"
#include "kmdw_tof.h"
#include "kmdw_jpeg.h"
#include "flatbuffer_setup_reader.h"
#include "math.h"
#include "kdrv_system.h"
#include "kmdw_system.h"

#define DEBUG 0
#define FLAG_KMDW_MODEL_ABORT      BIT(29)  // Event flag to notify abort
#define FLAG_KMDW_MODEL_FROM_NCPU  BIT(30)  // Event flag to know NCPU is done
#define FLAG_KMDW_MODEL_FROM_NPU   BIT(28)  // Event flag to know NPU is done

#define MODEL_INF_TIMEOUT           (2000) // 2 secs timeout OK ? FIXME

#define KDP_FLASH_FW_INFO_SIZE      0x1000

extern const struct s_kdp_memxfer kdp_memxfer_module;

/**
 * KL720 NPU data format
 * note: from kl720 firmware kneron_beethoven_dsp.h
 */
enum {
    FMT_1W16C8B,
    FMT_1W16C8B_INTLV,
    FMT_1W16C8BHL,
    FMT_1W16C8BHL_INTLV,
    FMT_4W4C8B,
    FMT_16W1C8B,
    FMT_8W1C16B,
    FMT_RAW8 = 100,
    FMT_RAW16 = 101,
    FMT_RAW_FLOAT = 102,
    FMT_ONODE_MAX
};

/**
 * setup.bin schema: model node type
 * note: from kl520 firmware kdpio.h to parse setup.bin
 */
enum
{
    NODE_TYPE_IN,
    NODE_TYPE_CPU,
    NODE_TYPE_OUTPUT,
    NODE_TYPE_DATA,
    NODE_TYPE_SUPER,
    NODE_TYPE_INPUT,
};

// CNN_Header and NetInput_Node are from kneron_api_data.h in ncpu firmware to parse setup.bin
typedef struct CNN_Header {
    uint32_t crc;
    uint32_t version;
    uint32_t reamaining_models;
    uint32_t model_type;
    uint32_t application_type;
    uint32_t dram_start;
    uint32_t dram_size;
    uint32_t cmd_start;
    uint32_t cmd_size;
    uint32_t weight_start;
    uint32_t weight_size;
    uint32_t input_start;
    uint32_t input_size;
    uint32_t input_num;
    uint32_t output_num;
} CNN_Header;

// node_id = 0
typedef struct In_Node {
    uint32_t node_id;
    uint32_t next_npu;
} In_Node;

// node_id = 4
typedef struct Super_Node {
    uint32_t node_id;
    uint32_t addr;
    uint32_t row_start;
    uint32_t col_start;
    uint32_t ch_start;
    uint32_t row_length;
    uint32_t col_length;
    uint32_t ch_length;
} Super_Node;

// node_id = 3
typedef struct Data_Node {
    uint32_t node_id;
    uint32_t supernum;
    uint32_t data_format;
    uint32_t data_radix;
    uint32_t data_scale;
    uint32_t row_start;
    uint32_t col_start;
    uint32_t ch_start;
    uint32_t row_length;
    uint32_t col_length;
    uint32_t ch_length;
    Super_Node* node_list;
} Data_Node;

// node_id = 1
typedef struct CPU_Node {
    uint32_t node_id;
    uint32_t input_datanode_num;
    uint32_t op_type;
    uint32_t in_num_row;
    uint32_t in_num_col;
    uint32_t in_num_ch;
    uint32_t out_num_row;
    uint32_t out_num_col;
    uint32_t out_num_ch;
    uint32_t h_pad;
    uint32_t w_pad;
    uint32_t kernel_h;
    uint32_t kernel_w;
    uint32_t stride_h;
    uint32_t stride_w;
    Data_Node* output_datanode;
    Data_Node* input_datanodes;
} CPU_Node;

// node_id = 2
typedef struct Out_Node {
    uint32_t node_id;
    uint32_t supernum;
    uint32_t data_format;
    uint32_t row_start;
    uint32_t col_start;
    uint32_t ch_start;
    uint32_t row_length;
    uint32_t col_length;
    uint32_t ch_length;
    uint32_t output_index;
    uint32_t output_radix;
    uint32_t output_scale;
    Super_Node* node_list;
} Out_Node;

// node_id = 5
typedef struct NetInput_Node {
    uint32_t node_id;
    uint32_t input_index;
    uint32_t input_format;
    uint32_t input_row;
    uint32_t input_col;
    uint32_t input_channel;
    uint32_t input_start;
    uint32_t input_size;
    uint32_t input_radix;
} NetInput_Node;

typedef struct {
    uint32_t n_model_source;           // 0: not set, 1: from flash, 2: from ddr
    uint32_t n_model_count;            // model count
    struct kdp_model_s p_model_info[KMDW_MODEL_MAX_MODEL_COUNT];  // save model info generated by compiler
    uint8_t pn_is_model_loaded_table[KMDW_MODEL_MAX_MODEL_COUNT]; // flag table to indicate if model is loaded
    uint32_t n_ddr_addr_model_end;     // DDR address of model end = user data start
    int32_t n_model_slot_index;        // scpu_to_ncpu->model_slot_index

    uint32_t    parallel_output_addr[KMDW_MODEL_MAX_MODEL_COUNT];
} kmdw_model_data_t;

kmdw_model_data_t s_model_data = {0};

typedef struct {
    int32_t raw_img_idx;
    osEventFlagsId_t evt_caller;        // event to know/control ncpu
    uint32_t        caller_e;
    osEventFlagsId_t evt_result;        // event to know/control npu
    uint32_t        result_e;
} kmdw_img_data_t;

// ptr to the buf for uploaded fw info from host
static kmdw_model_fw_info_t *s_fw_info_buf_p = NULL;
// bool to confirm whether model has been loaded from flash
static bool s_model_loaded_from_flash = false;

static kmdw_img_data_t s_img_data[IPC_IMAGE_ACTIVE_MAX] = {0};
static int32_t s_current_ipc_idx = 0;
static int32_t s_next_ipc_idx = 0;
static int32_t model_paralleled[KMDW_MODEL_MAX_MODEL_COUNT] = {0};

uint32_t ncpu_reset_cnt;
/*  jpeg related definitions   */
#ifdef KDP2_FW
extern jpeg_enc_kmdw_ctx_t jpeg_enc_kmdw_ctx;
extern jpeg_dec_kmdw_ctx_t jpeg_dec_kmdw_ctx;
#endif

extern tof_dec_kmdw_ctx_t tof_dec_kmdw_ctx;
extern ir_bright_kmdw_ctx_t ir_bright_kmdw_ctx;

#if APP_STEREO_DEPTH_PROJECT
extern struct model_stereo_depth_ctx_s stereo_depth_kmdw_ctx;
#endif

/* ############################
 * ##    Static Functions    ##
 * ############################ */

/**
 * @brief init ddr space for s_fw_info_buf_p
 *
 */
static void _init_fw_info_buf(void)
{
    if (NULL == s_fw_info_buf_p) {
        s_fw_info_buf_p = (kmdw_model_fw_info_t*)kmdw_ddr_reserve(KDP_FLASH_FW_INFO_SIZE);

        if (NULL == s_fw_info_buf_p)
            critical_msg("insufficent memory for reading fw_info from flash\n");
    }
}

/**
 * @brief load fw info from flash to ddr
 * @return address of ddr
 */
static kmdw_model_fw_info_t* _load_flash_model_info(void)
{
    // load model from flash once and reuse loaded data, until reload

    if (false == s_model_loaded_from_flash) {
        s_model_loaded_from_flash = true;
        kdp_memxfer_module.flash_to_ddr((uint32_t)s_fw_info_buf_p, FLASH_MODEL_FW_INFO_ADDR, KDP_FLASH_FW_INFO_SIZE);
    }

    return s_fw_info_buf_p;
}

/**
 * @brief set s_model_data.parallel_output_addr and n_ddr_addr_model_end
 * @return 0: OK, -1: fail
 */
static int32_t _set_parallel_output_addr_and_ddr_end(void)
{
    int i;
    uint32_t addr, len, len_max = 0;

    if (s_model_data.n_model_count >= KMDW_MODEL_MAX_MODEL_COUNT) {
        err_msg("model count %d exceeds max %d\n",
                s_model_data.n_model_count, KMDW_MODEL_MAX_MODEL_COUNT);
        return 0;
    }

    addr = ALIGN64(s_model_data.n_ddr_addr_model_end);
    len = 0;
    for (i = 0; i < s_model_data.n_model_count; i++) {
        addr = ALIGN64(addr + len);
        s_model_data.parallel_output_addr[i] = addr;

        len = s_model_data.p_model_info[i].output_mem_len;
        if (len > len_max )
            len_max = len;
        dbg_msg("model[%d]: parallel buffer: 0x%x (len 0x%x)\n", i, addr, len);

        // Reset parallel handling
        model_paralleled[i] = 0;
    }
    dbg_msg("max parallel buffer: 0x%x (len 0x%x)\n", addr, len_max);
    addr = ALIGN64(addr + len_max);

    if (addr >= kmdw_ddr_get_heap_tail()) {
        err_msg("model: parallel buffer: 0x%x over (>=) boundary 0x%x\n", addr, kmdw_ddr_get_heap_tail());
        return -1;
    } else {
        dbg_msg("model & info DDR end: 0x%x (0x%x)\n", addr, s_model_data.n_ddr_addr_model_end);
    }

    s_model_data.n_ddr_addr_model_end = addr;
    return 0;
}

/**
 * @brief reset s_model_data
 */
static void _reset_model_data(void)
{
    s_model_data.n_model_count = 0;
    s_model_data.n_model_source = 0;
    memset( s_model_data.p_model_info, 0, sizeof(s_model_data.p_model_info));
    memset( s_model_data.pn_is_model_loaded_table, 0, sizeof(s_model_data.pn_is_model_loaded_table));

    return;
}

/**
 * @brief convert modeltype to modelInfo array index
 * @param model_type_p: model type (defined in model_type.h)
 * @return modelInfo model index (starts from 0)
 *         -1 means not such modeltype in flash
 */
static int8_t _get_model_info_array_index_by_model_type(uint32_t model_type_p)
{
    int i;
    for(i=0 ; i < s_model_data.n_model_count; i++) {
        if(s_model_data.p_model_info[i].model_type == model_type_p)
            return i;
    }

    return -1;
}

/**
 * @brief get fw info extension data from fw_info ptr
 * @param[in] fw_info_p the ptr to fw_info
 * @return the ptr to fw_info_ext
 */
static kmdw_model_fw_info_ext_t*
_get_fw_info_ext_by_fw_info(kmdw_model_fw_info_t* fw_info_p)
{
    if(NULL == fw_info_p)
        return NULL;
    else {
        kmdw_model_fw_info_ext_t* ret = NULL;
        uint32_t count;
        uint32_t offset;

        count = fw_info_p->model_count;
        offset = sizeof(struct kdp_model_s) * count;
        ret = (kmdw_model_fw_info_ext_t *)((uint32_t)fw_info_p->models + offset);
        return ret;
    }
}

/**
 * @brief load model information generated by compiler
 * @param [in] is_model_from_ddr: if model is from ddr/host command
 * @param [in] is_reload        : is force reload
 * @return model count
 *         0 means no model is loaded in this call
 */
static int32_t _load_model_info(bool from_ddr, bool reload)
{
    if (s_model_data.n_model_count && !reload) {
        return s_model_data.n_model_count;
    }

    if (reload) {
        _reset_model_data();
        s_model_loaded_from_flash = false;
    }

    kmdw_model_fw_info_t *model_info_p = NULL;
    kmdw_model_fw_info_ext_t *model_info2_p = NULL;

    // load model Info
    if (from_ddr) {
        model_info_p = s_fw_info_buf_p;
        model_info2_p = _get_fw_info_ext_by_fw_info(model_info_p);

        if((NULL == model_info_p) || (NULL == model_info2_p) ) {
            s_model_data.n_model_count = 0;
            return 0;
        }

        // get model count
        s_model_data.n_model_count = model_info_p->model_count;
        dbg_msg("model info: model count:%d\n", s_model_data.n_model_count);

        if(0 == s_model_data.n_model_count) {
            info_msg("model is not in DDR!!\n");
            return 0;
        } else if (s_model_data.n_model_count > KMDW_MODEL_MAX_MODEL_COUNT) {
            info_msg("model count is over MAX limit=%d!!\n", KMDW_MODEL_MAX_MODEL_COUNT);
            s_model_data.n_model_count = 0;
            return 0;
        } else {
            dbg_msg("model info: model count:%d\n", s_model_data.n_model_count);
        }

        // get model info
        memcpy(s_model_data.p_model_info, (const void*)model_info_p->models,
               sizeof(struct kdp_model_s)*s_model_data.n_model_count);

        // get ddr model end addr
        s_model_data.n_ddr_addr_model_end = model_info2_p->model_dram_addr_end;

        if (s_model_data.n_ddr_addr_model_end >= kmdw_ddr_get_heap_tail()) {
            err_msg("modelInfo: DDR end address: 0x%x over (>=) boundary 0x%x\n", s_model_data.n_ddr_addr_model_end, kmdw_ddr_get_heap_tail());
            return 0;
        } else {
            dbg_msg("modelInfo: DDR end address: 0x%x\n", s_model_data.n_ddr_addr_model_end);
        }

        // set model source
        s_model_data.n_model_source = 2; // from ddr
    } else { // models are stored in flash

        model_info_p = _load_flash_model_info();  // this function updates data on s_fw_info_buf_p
        model_info2_p = _get_fw_info_ext_by_fw_info(model_info_p);

        if((NULL == model_info_p) || (NULL == model_info2_p) ) {
            s_model_data.n_model_count = 0;
            return 0;
        }

        // get model count
        s_model_data.n_model_count = model_info_p->model_count;
        dbg_msg("model info: model count:%d\n", s_model_data.n_model_count);

        if (s_model_data.n_model_count == 0xFFFFFFFF) {
            err_msg("model is not in flash!!\n");
            s_model_data.n_model_count = 0;
            return 0;
        } else if (s_model_data.n_model_count > KMDW_MODEL_MAX_MODEL_COUNT) {
            info_msg("model count is over MAX limit=%d!!\n", KMDW_MODEL_MAX_MODEL_COUNT);
            s_model_data.n_model_count = 0;
            return 0;
        } else {
            dbg_msg("model info: model count:%d\n", s_model_data.n_model_count);
        }

        // get model info
        memcpy(s_model_data.p_model_info, model_info_p->models, sizeof(struct kdp_model_s)*s_model_data.n_model_count);

        // get ddr model end addr
        s_model_data.n_ddr_addr_model_end = model_info2_p->model_dram_addr_end;

        if (s_model_data.n_ddr_addr_model_end >= kmdw_ddr_get_heap_tail()) {
            err_msg("modelInfo: DDR end address: 0x%x over (>=) boundary 0x%x\n", s_model_data.n_ddr_addr_model_end, kmdw_ddr_get_heap_tail());
            return 0;
        } else {
            dbg_msg("modelInfo: DDR end address: 0x%x\n", s_model_data.n_ddr_addr_model_end);
        }

        // set model source
        s_model_data.n_model_source = 1; // from flash
    }

    int ret = _set_parallel_output_addr_and_ddr_end();
    if(-1 == ret)
        return 0;

    return s_model_data.n_model_count;
}

/**
 * @brief load specific model by model info index (the order in flash)
 * @param model_index_p: model info index
 * @return 0: model not ready, 1: model is loaded
 */
static int32_t _load_model(uint8_t model_index_p/*starts from 0*/)
{
    uint32_t ddr_addr_models_head; //start point = the 1st model's cmd.bin
    uint32_t ddr_addr_offset;
    uint32_t flash_addr;
    uint32_t len_to_load;
    struct kdp_model_s *p_model;

    if(s_model_data.n_model_count == 0)
        return 0; // model info is not ready

    if(s_model_data.pn_is_model_loaded_table[model_index_p] == 1 )
        return 1; //model has been loaded
    else {
        //load model with (index=model_index_p) from flash to DDR
        ddr_addr_models_head = s_model_data.p_model_info[0].cmd_mem_addr; //start point = the 1st model's cmd.bin

        //load cmd + weight + setup together
        p_model = &(s_model_data.p_model_info[model_index_p]);
        ddr_addr_offset = p_model->cmd_mem_addr - ddr_addr_models_head;

        flash_addr = FLASH_MDDEL_ALL_ADDR + ddr_addr_offset;

        len_to_load = ALIGN16(p_model->cmd_mem_len) +
                      ALIGN16(p_model->weight_mem_len) +
                      ALIGN16(p_model->setup_mem_len);

        //model from flash to ddr
        kdp_memxfer_module.flash_to_ddr(p_model->cmd_mem_addr, flash_addr, len_to_load);

        s_model_data.pn_is_model_loaded_table[model_index_p] = 1;
    }

    return 1;
}


/**
 * @brief specify model information, load model info, load model
 * @param [in] model_type_p: model unique ID defined by Kneron
 * @param [in] model_from_ddr: is model from ddr or host command
 * @return model_slot_index(requested by NCPU/NPU)
 *         -1 : model not found
 */
static int32_t _config_model(uint32_t model_type, bool model_from_ddr)
{
    int model_info_idx; //limitation (hard coded in flash)
    int model_idx;

    //check if model info is loaded
    if( 0 == _load_model_info(model_from_ddr, false/*reload*/)) {
        return -1;
    }

    if( model_from_ddr == 0 ) {
        //FIXME, should remove application related code
        /* Special model not in DDR but in ncpu */
        if (model_type == KNERON_2D_LIVENESS_V3_FACEBAGNET_224_224_3) {
            model_idx = 3;
            model_info_idx = 4;

            goto model_common;
        }

        model_info_idx = _get_model_info_array_index_by_model_type(model_type);
        if(model_info_idx == -1) {
            err_msg("model_type[%d] is not found in flash\n", model_type);
            return -1;
        }
        _load_model(model_info_idx);

        // FIXME: need to remove the following hard code
        model_idx = model_info_idx;

    } else {
        model_info_idx = _get_model_info_array_index_by_model_type(model_type);
        if(model_info_idx == -1) {
            err_msg("model_type[%d] is not found in DDR\n", model_type);
            return -1;
        }
        model_idx = model_info_idx;
    }

model_common:
    s_model_data.n_model_slot_index = model_idx;

    kmdw_ipc_set_model(s_model_data.p_model_info, model_info_idx, model_idx);

    struct kdp_img_raw_s *raw_img = kmdw_model_get_raw_img(s_img_data[s_current_ipc_idx].raw_img_idx);

    if (raw_img->inf_format & IMAGE_FORMAT_PARALLEL_PROC) {
        int mirrored_idx = IPC_MODEL_MAX - model_idx - 1;      // mirror for parallel

        if (!model_paralleled[model_idx]) {

            model_paralleled[model_idx] = 1;

            /* Duplicate model_info and use parallel output buffer */
            struct scpu_to_ncpu_s* comm_out = kmdw_ipc_get_output();
            struct kdp_model_s *p_mirror_model_info = s_model_data.p_model_info;
            comm_out->models[mirrored_idx] = *(p_mirror_model_info + model_idx);
            comm_out->models_type[mirrored_idx] = (p_mirror_model_info + model_idx)->model_type;

            uint32_t len = comm_out->models[mirrored_idx].output_mem_len;
            comm_out->models[mirrored_idx].output_mem_addr = s_model_data.parallel_output_addr[model_idx];

            info_msg("Parallel buffer: model %d [%d %d], len %d, addr 0x%x\n",
                     model_type, model_idx, mirrored_idx, len,
                     comm_out->models[mirrored_idx].output_mem_addr);
        }

        if (s_current_ipc_idx != 0)
            model_idx = mirrored_idx;
    }

    struct scpu_to_ncpu_s* p_comm_out = kmdw_ipc_get_output();

    if (NULL == p_comm_out->output_mem_addr3) {
        uint32_t len = 0x5000;
        p_comm_out->output_mem_addr3 = kmdw_ddr_reserve(len*sizeof(uint32_t));
        if(NULL == p_comm_out->output_mem_addr3) {
            critical_msg("kmdw_model: failed to malloc comm_out->output_mem_addr3\n");
            return -1;
        }
    }

    if (NULL == p_comm_out->output_mem_addr4) {
        uint32_t len = 8 * (1 << 20);
        p_comm_out->output_mem_addr4 = kmdw_ddr_reserve(len);
        if(NULL == p_comm_out->output_mem_addr4) {
            critical_msg("kmdw_model: failed to malloc comm_out->output_mem_addr4\n");
            return -1;
        }
        info_msg("CenterNet/Yolo5 (%d) buffer: 0x%x (len 0x%x)\n", model_type, p_comm_out->output_mem_addr4, len);
    }

    kmdw_ipc_set_model_active(model_idx);

    return model_idx;
}

/**
 * @brief run model according to config settings
 * @return status defined in NCPU
 * @note !!! must be called after kapp_config_model_image()
 */
static int32_t _run_model(void)
{
    int active_idx = s_current_ipc_idx;
    int raw_img_idx = s_img_data[active_idx].raw_img_idx;
    struct kdp_img_raw_s *p_raw_image = kmdw_model_get_raw_img(raw_img_idx);
    uint32_t flags, wait_evt;
    uint32_t is_abort = 0;

    // Start time for ncpu/npu round trip
    p_raw_image->tick_start = osKernelGetSysTimerCount();

    if (s_img_data[active_idx].evt_caller == NULL)
        s_img_data[active_idx].evt_caller = osEventFlagsNew(0);

    // set notify for job done
    if (s_img_data[active_idx].evt_result) {
        /* Result event already set. Let's do local event for parallel. */
        wait_evt = FLAG_KMDW_MODEL_FROM_NPU;
    } else {
        wait_evt = FLAG_KMDW_MODEL_FROM_NCPU;
    }

    dbg_msg("<_run_model> wait %d[%d] evt %x\n", raw_img_idx, active_idx, wait_evt);

    //assign caller event before triggering ncpu/npu
    s_img_data[active_idx].caller_e = wait_evt;

    //trigger ncpu/npu
    kmdw_ipc_trigger_int(CMD_RUN_NPU);

    p_raw_image->tick_got_ncpu_ack = osKernelGetSysTimerCount();

    //check abort signal
    flags = osEventFlagsWait(s_img_data[active_idx].evt_caller,
                             FLAG_KMDW_MODEL_ABORT,
                             osFlagsWaitAll, 0);
    if( flags != osFlagsErrorResource ) {
        osEventFlagsClear(s_img_data[active_idx].evt_caller, FLAG_KMDW_MODEL_ABORT);
        is_abort = 1;
    }

    //wait for finish of current task
    uint32_t wait_timeout = (kmdw_ipc_get_output()->kp_dbg_checkpoinots == 0x0) ? MODEL_INF_TIMEOUT : osWaitForever;

    flags = osEventFlagsWait(s_img_data[active_idx].evt_caller,
                             wait_evt,
                             osFlagsNoClear, wait_timeout);

    if(flags == osFlagsErrorTimeout) {
        err_msg("[%s] osEventFlagsWait flag 0x%08x timeout\n", __FUNCTION__, wait_evt);
        kdrv_system_reset(SUBSYS_NCPU);
        ncpu_reset_cnt++;
        load_ncpu_fw(1);
        osDelay(1000);
        return IMAGE_STATE_TIMEOUT;
    } else if (flags != wait_evt)
        dbg_msg("[%s] 1+ events 0x%08x (%d[%d] expected)\n", __FUNCTION__, flags, wait_evt, active_idx);
    else
        dbg_msg("[%s] got: raw_img_idx[active_idx]=%d[%d]\n", __FUNCTION__, raw_img_idx, active_idx);

    osEventFlagsClear(s_img_data[active_idx].evt_caller, wait_evt);

    if( 1 == is_abort ) {
        dbg_msg("[%s] abort after n_model_slot_index = %d\n", __FUNCTION__, s_model_data.n_model_slot_index);
        return KMDW_MODEL_RUN_RC_ABORT; //abort
    }

    //FIXME, should return sts
    return 0;
}

#ifndef KDP2_FW
// sorry, due to extern variable
extern uint16_t APP_id;
#endif

__weak osStatus_t kmdw_fifoq_manager_result_enqueue(void *result_buf, int buf_size, bool preempt)
{
    //Warnning, here is handler mode
    return osOK;
}

static void _ipc_handler(struct kdp_img_raw_s *p_raw_image, int state)
{
    int ipc_idx;

    if (state == 0x999) // FIXME, very workaround
    {
        kmdw_ipc_get_input()->kp_dbg_status = 0x0;
        osStatus_t sts = kmdw_fifoq_manager_result_enqueue(kmdw_ipc_get_output()->kp_dbg_buffer, KP_DEBUG_BUF_SIZE, false);
        if(sts != osOK)
            ipc_err_msg("send dbg data failed in ipc, err %d\n", sts);
    }
    else if (state == IMAGE_STATE_RECEIVING) {
        ipc_idx = p_raw_image->ref_idx;

        // End time for ncpu/npu round trip
        p_raw_image->tick_end = osKernelGetSysTimerCount();

        if (s_img_data[ipc_idx].evt_result) {
            ipc_dbg_msg("[done: post: P] ipc_idx: %d, result_e: %d (ram %x)\n", ipc_idx, s_img_data[ipc_idx].result_e, p_raw_image);
            osEventFlagsSet(s_img_data[ipc_idx].evt_result, s_img_data[ipc_idx].result_e);
        } else {
            ipc_dbg_msg("[done: post: S] ipc_idx: %d, caller_e: %x.\n", ipc_idx, s_img_data[ipc_idx].caller_e);
            osEventFlagsSet(s_img_data[ipc_idx].evt_caller, s_img_data[ipc_idx].caller_e);
        }
    } else if (state == IMAGE_STATE_ACTIVE) {
        ipc_idx = s_current_ipc_idx;
        ipc_dbg_msg("[done: npu: P] ipc_idx: %d, caller_e: %x\n", ipc_idx, s_img_data[ipc_idx].caller_e);
        osEventFlagsSet(s_img_data[ipc_idx].evt_caller, s_img_data[ipc_idx].caller_e);
    } else if (state == IMAGE_STATE_ERR_DSP_BUSY) {
        ipc_idx = p_raw_image->ref_idx;
        // End time for ncpu/npu round trip
        p_raw_image->tick_end = osKernelGetSysTimerCount();

        if (s_img_data[ipc_idx].evt_result) {
            // printf("IMAGE_STATE_ERR_DSP_BUSY [done: post: P] ipc_idx: %d, result_e: %d (ram %x)\n", ipc_idx, s_img_data[ipc_idx].result_e, p_raw_image);
            osEventFlagsSet(s_img_data[ipc_idx].evt_result, s_img_data[ipc_idx].result_e);
        } else {
            // printf("IMAGE_STATE_ERR_DSP_BUSY [done: post: S] ipc_idx: %d, caller_e: %x.\n", ipc_idx, s_img_data[ipc_idx].caller_e);
            osEventFlagsSet(s_img_data[ipc_idx].evt_caller, s_img_data[ipc_idx].caller_e);
        }
#ifdef KDP2_FW
    } else if( state == IMAGE_STATE_JPEG_ENC_DONE) {
        ncpu_to_scpu_result_t * p_in_ipc = jpeg_enc_kmdw_ctx.p_ipc_in;
        jpeg_encode_result_t  *pRes = (jpeg_encode_result_t  *)p_in_ipc->extRsltAddr;
        jpeg_enc_kmdw_ctx.npu_rslt = *pRes;
        if(jpeg_enc_kmdw_ctx.jpeg_enc_evt_id == 0) {
            ipc_err_msg("jpeg_enc_evt_id was not created\n");
            return;
        }
        osEventFlagsSet(jpeg_enc_kmdw_ctx.jpeg_enc_evt_id, jpeg_enc_kmdw_ctx.jpeg_enc_evt_flag);
    } else if( state == IMAGE_STATE_JPEG_DEC_DONE) {
        ncpu_to_scpu_result_t * p_in_ipc = jpeg_dec_kmdw_ctx.p_ipc_in;
        jpeg_decode_result_t  *pRes = (jpeg_decode_result_t  *)p_in_ipc->extRsltAddr;
        jpeg_dec_kmdw_ctx.npu_rslt = *pRes;
        if(jpeg_dec_kmdw_ctx.jpeg_dec_evt_id == 0) {
            ipc_err_msg("jpeg_dec_evt_id was not created\n");
            return;
        }
        osEventFlagsSet(jpeg_dec_kmdw_ctx.jpeg_dec_evt_id, jpeg_dec_kmdw_ctx.jpeg_dec_evt_flag);
#endif
#if TOF_ENABLED
    } else if( state == IMAGE_STATE_TOF_DEC_DONE) {
        ncpu_to_scpu_result_t * p_in_ipc = tof_dec_kmdw_ctx.p_ipc_in;
        tof_decode_result_t  *pRes = (tof_decode_result_t  *)p_in_ipc->extRsltAddr;
        tof_dec_kmdw_ctx.npu_rslt = *pRes;
        if(tof_dec_kmdw_ctx.tof_dec_evt_id == 0) {
            ipc_err_msg("tof_dec_evt_id was not created\n");
            return;
        }
        osEventFlagsSet(tof_dec_kmdw_ctx.tof_dec_evt_id, tof_dec_kmdw_ctx.tof_dec_evt_flag);
    } else if( state == IMAGE_STATE_TOF_CALC_IR_BRIGHT_DONE) {
        ncpu_to_scpu_result_t * p_in_ipc = ir_bright_kmdw_ctx.p_ipc_in;
        ir_bright_result_t  *pRes = (ir_bright_result_t  *)p_in_ipc->extRsltAddr;
        ir_bright_kmdw_ctx.npu_rslt = *pRes;
        if(ir_bright_kmdw_ctx.ir_bright_evt_id == 0) {
            ipc_err_msg("ir_bright_evt_id was not created\n");
            return;
        }
        osEventFlagsSet(ir_bright_kmdw_ctx.ir_bright_evt_id, ir_bright_kmdw_ctx.ir_bright_evt_flag);
#endif
#if APP_STEREO_DEPTH_PROJECT
    } else if( state == IMAGE_STATE_STEREO_DEPTH_DONE) {
        ncpu_to_scpu_result_t * p_in_ipc = stereo_depth_kmdw_ctx.p_ipc_in;
        stereo_depth_result_t  *pRes = (stereo_depth_result_t  *)p_in_ipc->extRsltAddr;
        stereo_depth_kmdw_ctx.npu_rslt = *pRes;
        if(stereo_depth_kmdw_ctx.stereo_depth_evt_id == 0) {
            printf("stereo_depth_evt_id was not created\n");
            return;
        }
        osEventFlagsSet(stereo_depth_kmdw_ctx.stereo_depth_evt_id, stereo_depth_kmdw_ctx.stereo_depth_evt_flag);
#endif
    } else {
        ipc_err_msg("wrong state: %d\n", state);
    }
}

/**
 * @brief round up with specified number
 * @param [in] num: source number
 * @param [in] round_num: specified round up number
 * @return round up number
 * @note for example: _round_up_with_num(0, 3) = 3; _round_up_with_num(4, 3) = 6;
 */
static uint32_t _round_up_with_num(uint32_t num, uint32_t round_num) {
    return ((num + (round_num - 1)) & ~(round_num - 1));
}


/* ############################
 * ##    Public Functions    ##
 * ############################ */

void kmdw_model_init(void)
{
    kmdw_ipc_initialize(_ipc_handler);
#ifdef KDP2_FW
    memset( (void *)&jpeg_enc_kmdw_ctx, 0, sizeof(jpeg_enc_kmdw_ctx));
#endif
#if TOF_ENABLED
    memset( (void *)&tof_dec_kmdw_ctx, 0, sizeof(tof_dec_kmdw_ctx));
    memset( (void *)&ir_bright_kmdw_ctx, 0, sizeof(ir_bright_kmdw_ctx));
#endif
#if APP_STEREO_DEPTH_PROJECT
    memset( (void *)&stereo_depth_kmdw_ctx, 0, sizeof(stereo_depth_kmdw_ctx));
#endif
#ifndef KDP2_FW
// sorry, due to extern variables
    jpeg_4M_reserved_blk = 0;
    jpeg_2M_reserved_blk = 0;
#endif

    _init_fw_info_buf();

    s_fw_info_buf_p->model_count = 0;
}

int32_t kmdw_model_load_model(int8_t model_info_index_p)
{
    int32_t ret = 0;

    if(1 != s_model_data.n_model_source ||  // check if s_model_data is not according to flash
       0 == s_model_data.n_model_count) {
        if(0 ==  _load_model_info(false/*from ddr*/, true/*reload*/))
            return 0; //error, no model is loaded
    }

    // load all models
    if (KMDW_MODEL_ALL_MODELS == model_info_index_p) {
        uint8_t i;
        for (i = 0 ; i < s_model_data.n_model_count ; i++) {
            ret = _load_model(i);
            if( 0 == ret) {
                err_msg("[%s] : failed to load model array index:%d\n", __FUNCTION__, i);
                return 0;
            }
        }

// Very slow if turn it on. Maybe hardware support is needed.
// Add a new compiler directive if CRC32 method is also used in other scenarios (ex: check FW image)
#if ENABLE_CRC32
        // check CRC value of all_models.bin
        kmdw_model_fw_info_t *model_info_p = _load_flash_model_info();
        kmdw_model_fw_info_ext_t *model_info2_p = _get_fw_info_ext_by_fw_info(model_info_p);

        // cmd_mem_addr of first model is the start address of all_models.bin
        uint8_t *addr = (uint8_t *)s_model_data.p_model_info[0].cmd_mem_addr;

        uint32_t crc32 = kmdw_utils_crc_gen_crc32(addr, model_info2_p->model_total_size);

        dbg_msg("[%s] crc32 calculated: 0x%x\n", __FUNCTION__, crc32);
        dbg_msg("[%s] crc32 read from flash: 0x%x\n", __FUNCTION__, model_info2_p->model_checksum);
        dbg_msg("[%s] model start address: 0x%x\n", __FUNCTION__, s_model_data.p_model_info[0].cmd_mem_addr);
        dbg_msg("[%s] model total size: %d\n", __FUNCTION__, model_info2_p->model_total_size);

        if (crc32 != model_info2_p->model_checksum)
        {
            err_msg("[%s]: all models.bin CRC check failed\n", __FUNCTION__);
            return 0;
        }
#endif

        return s_model_data.n_model_count;
    } else { // load specific model
        ret = _load_model(model_info_index_p);
        return ret;
    }
}

int32_t kmdw_model_reload_model_info(bool from_ddr)
{
    return _load_model_info(from_ddr, true/*reload*/);
}

int32_t kmdw_model_refresh_models(void)    // reload all the models from flash again
{
    uint8_t i;

    // forcedly update s_model_data which might be poluted by model upload from host
    if(0 ==  _load_model_info(false/*from ddr*/, true/*reload*/))
        return 0; //error, no model is loaded

    int ret;
    for (i = 0 ; i < s_model_data.n_model_count ; i++) {
        if (s_model_data.pn_is_model_loaded_table[i]) {  // if previously loaded
            s_model_data.pn_is_model_loaded_table[i] = 0;
            ret = _load_model(i);  // reload the model again
            if ( 0 == ret) {
                err_msg("[%s] : failed to load model array index:%d\n", __FUNCTION__, i);
                return 0;
            }
        }
    }
    return s_model_data.n_model_count;
}

int32_t kmdw_model_config_result(osEventFlagsId_t result_evt, uint32_t result_evt_flag)
{
    int active_idx = s_current_ipc_idx;

    s_img_data[active_idx].evt_result = result_evt;
    s_img_data[active_idx].result_e = result_evt_flag;

    return 0;
}

void kmdw_model_config_img(struct kdp_img_cfg *img_cfg, void *ext_param)
{
    int act_img_idx = img_cfg->image_buf_active_index;
    struct kdp_img_raw_s *raw_img = kmdw_model_get_raw_img(act_img_idx);

    s_current_ipc_idx = s_next_ipc_idx;

    if (img_cfg->inf_format & IMAGE_FORMAT_PARALLEL_PROC)
        s_next_ipc_idx = !s_next_ipc_idx;

    kmdw_ipc_set_image_active(act_img_idx);
    s_img_data[s_current_ipc_idx].raw_img_idx = act_img_idx;

    raw_img->state = IMAGE_STATE_ACTIVE;
    raw_img->seq_num = act_img_idx;

    raw_img->ref_idx = s_current_ipc_idx;
    raw_img->num_image = img_cfg->num_image;
    raw_img->inf_format = img_cfg->inf_format;

    for (int i = 0; i < img_cfg->num_image; i++) {
        raw_img->image_list[i].input_row = img_cfg->image_list[i].input_row;
        raw_img->image_list[i].input_col = img_cfg->image_list[i].input_col;
        raw_img->image_list[i].input_channel = img_cfg->image_list[i].input_channel;
        raw_img->image_list[i].format = img_cfg->image_list[i].format;
        raw_img->image_list[i].image_mem_addr = img_cfg->image_list[i].image_mem_addr;
        raw_img->image_list[i].image_mem_len = img_cfg->image_list[i].image_mem_len;

        memcpy(&(raw_img->image_list[i].params_s), &(img_cfg->image_list[i].params_s), sizeof(parameter_t));
    }

    if (ext_param == NULL) {
        memset(raw_img->ext_params, 0, MAX_PARAMS_LEN * 4);
    } else {
        memcpy(raw_img->ext_params, ext_param, MAX_PARAMS_LEN * 4);
    }
}

struct kdp_img_raw_s* kmdw_model_get_raw_img(int idx)
{
    struct scpu_to_ncpu_s *comm_out = kmdw_ipc_get_output();
    return &(comm_out->raw_images[idx]);
}

int kmdw_model_run(const char *tag, void *output, uint32_t model_type, bool model_from_ddr)
{
    int model_idx = _config_model(model_type, model_from_ddr);
    if (model_idx < 0) {
        return KMDW_MODEL_RUN_RC_ABORT;
    }

    int img_idx = s_img_data[s_current_ipc_idx].raw_img_idx;
    struct kdp_img_raw_s *raw_img = kmdw_model_get_raw_img(img_idx);

//    raw_img->results[model_idx].result_mem_addr = (uint32_t)output;
    raw_img->result.result_mem_addr = (uint32_t)output;

    dbg_msg("tag:%s:\n", tag);
    dbg_msg("  model_idx = %d\n", model_idx);
    dbg_msg("  model type = %d\n", model_type);
    dbg_msg("  ref_idx = %d\n", raw_img->ref_idx);
    dbg_msg("  inf_format = 0x%X\n", raw_img->inf_format);
    dbg_msg("  output addr = 0x%x\n", raw_img->result.result_mem_addr);
    dbg_msg("  ext_params(first 4)= %d/%d/%d/%d\n", raw_img->ext_params[0], raw_img->ext_params[1],
                                                    raw_img->ext_params[2], raw_img->ext_params[3]);

    for (int i = 0; i < raw_img->num_image; i++) {
        dbg_msg("  image index: %d\n", i);
        dbg_msg("    (row/col/ch) = %d/%d/%d\n", raw_img->image_list[i].input_row,
                                                 raw_img->image_list[i].input_col,
                                                 raw_img->image_list[i].input_channel);
        dbg_msg("    image format = 0x%x\n", raw_img->image_list[i].format);
        dbg_msg("    crop(tp/bt/lf/rt) = %d/%d/%d/%d\n", raw_img->image_list[i].params_s.crop_top,
                                                         raw_img->image_list[i].params_s.crop_bottom,
                                                         raw_img->image_list[i].params_s.crop_left,
                                                         raw_img->image_list[i].params_s.crop_right);
        dbg_msg("    image addr = 0x%x\n", raw_img->image_list[i].image_mem_addr);
    }

#if ((defined TDC_HW_PROTECTION_DFS) && (TDC_HW_PROTECTION_DFS == 1))
    if(kmdw_dfs_is_algorithm_running()){
        kmdw_dfs_set_npu_frequency();
    }
#endif
    return _run_model();
}

void kmdw_model_abort(void)
{
    int active_idx = s_current_ipc_idx;

    if( 0 == s_img_data[active_idx].evt_caller)
        return;

    osEventFlagsSet(s_img_data[active_idx].evt_caller, FLAG_KMDW_MODEL_ABORT);
}

struct kdp_model_s* kmdw_model_get_model_info(int model_idx_p)
{
    if (s_model_data.n_model_count == 0) {
        return NULL;
    } else if (model_idx_p >= s_model_data.n_model_count) {
        return NULL;
    } else {
        return &(s_model_data.p_model_info[model_idx_p]);
    }
}

void kmdw_model_get_run_time(int img_idx, kmdw_model_run_time_t *run_time/*out*/)
{
    uint32_t  freq_us;
    struct kdp_img_raw_s *p_raw_image;
    static uint32_t tick_prev_end = 0;

    if (run_time == NULL)
        return;


    p_raw_image = kmdw_model_get_raw_img(img_idx);
    freq_us = osKernelGetSysTimerFreq()/1000000;
    run_time->round_trip_time = (p_raw_image->tick_end - p_raw_image->tick_start)/(freq_us*1000.0);
    run_time->rslt_pop_interval = (p_raw_image->tick_end - tick_prev_end)/(freq_us*1000.0);
    run_time->scpu_send_ipc_ack = (p_raw_image->tick_got_ncpu_ack - p_raw_image->tick_start)/(freq_us*1000.0);
#ifdef PERF_HIGH_ACCURACY
    run_time->pre_proc_time = (p_raw_image->tick_end_pre - p_raw_image->tick_start_pre)*1.0/NCPU_CLOCK_CNT_PER_MS;
    run_time->npu_proc_time = (p_raw_image->tick_cnn_interrupt_rvd - p_raw_image->tick_start_npu)*1.0/NCPU_CLOCK_CNT_PER_MS;
    run_time->post_proc_time = (p_raw_image->tick_end_post - p_raw_image->tick_start_post)*1.0/NCPU_CLOCK_CNT_PER_MS;
    run_time->ncpu_req_ipc_ack = (p_raw_image->tick_ncpu_img_ack - p_raw_image->tick_ncpu_img_req)*1.0/NCPU_CLOCK_CNT_PER_MS;
    run_time->ncpu_rslt_ipc_ack = (p_raw_image->tick_rslt_got_scpu_ack - p_raw_image->tick_end_post)*1.0/NCPU_CLOCK_CNT_PER_MS;
    run_time->ncpu_img_req_to_rcv_new_img = (p_raw_image->tick_start_pre - p_raw_image->tick_last_img_req)*1.0/NCPU_CLOCK_CNT_PER_MS;
#else
    run_time->pre_proc_time = p_raw_image->tick_end_pre - p_raw_image->tick_start_pre;
    run_time->npu_proc_time = p_raw_image->tick_end_npu - p_raw_image->tick_start_npu;
    run_time->post_proc_time = p_raw_image->tick_end_post - p_raw_image->tick_start_post;
    run_time->ncpu_req_ipc_ack = p_raw_image->tick_ncpu_img_ack - p_raw_image->tick_ncpu_img_req;
    run_time->ncpu_rslt_ipc_ack = p_raw_image->tick_rslt_got_scpu_ack - p_raw_image->tick_end_post;
    run_time->ncpu_img_req_to_rcv_new_img = p_raw_image->tick_start_pre - p_raw_image->tick_last_img_req;
#endif
    tick_prev_end = p_raw_image->tick_end;
}

int kmdw_model_is_model_loaded(uint32_t model_type)
{
    if (_get_model_info_array_index_by_model_type(model_type) == -1)
        return 0;
    else
        return 1;
}

uint32_t *kmdw_model_get_all_model_info(bool trust_ddr_data)
{
    static uint32_t *s_p_model_id_list = NULL;  //[model_count, id0, id1, id2 ...]

    kmdw_model_fw_info_t *fw_info_ptr = NULL;

    fw_info_ptr = kmdw_model_get_fw_info(trust_ddr_data);

    if (fw_info_ptr) {

        if (NULL == s_p_model_id_list)
            s_p_model_id_list = (uint32_t *)calloc(1+KMDW_MODEL_MAX_MODEL_COUNT, sizeof(uint32_t));

        if (NULL == s_p_model_id_list) {
            err_msg("insufficent memory for model id list\n");
        } else {
            int i;
            uint32_t model_id;

            s_p_model_id_list[0] = fw_info_ptr->model_count;
            dbg_msg("%s:\n", __FUNCTION__);
            dbg_msg("Model Count = %d\n", s_p_model_id_list[0]);

            for (i = 0 ; i < s_p_model_id_list[0]; i++) {
                model_id = fw_info_ptr->models[i].model_type;
                dbg_msg("Extract Model ID %d\n", model_id);

                s_p_model_id_list[i+1] = model_id;
            }
        }
        return s_p_model_id_list;
    } else {
        return NULL;
    }

}

uint32_t kmdw_model_get_crc(bool trust_ddr_data)
{
    uint32_t ret = 0;
    kmdw_model_fw_info_t *fw_info_ptr;
    kmdw_model_fw_info_ext_t *fw_info_ext_ptr;

    fw_info_ptr = kmdw_model_get_fw_info(trust_ddr_data);
    fw_info_ext_ptr = _get_fw_info_ext_by_fw_info(fw_info_ptr);

    if (fw_info_ext_ptr) {
        ret = fw_info_ext_ptr->model_checksum;
    }

    dbg_msg("%s = 0x%x\n", __FUNCTION__, ret);

    return ret;
}

kmdw_model_fw_info_t* kmdw_model_get_fw_info(bool trust_ddr_data)
{
    uint32_t model_cnt;
    kmdw_model_fw_info_t *fw_info_ptr = s_fw_info_buf_p;

    if (false == trust_ddr_data) {
        if ((0 >= s_model_data.n_model_count) ||
            ((1 != s_model_data.n_model_source) && (2 != s_model_data.n_model_source))) {
            fw_info_ptr = NULL;
        } else {
            model_cnt = fw_info_ptr->model_count;
            if ((0 == model_cnt) || (model_cnt > KMDW_MODEL_MAX_MODEL_COUNT)) {
                fw_info_ptr = NULL;
            }
        }
    }

    return fw_info_ptr;
}

uint32_t kmdw_model_get_model_end_addr(bool trust_ddr_data)
{
    uint32_t ret = 0;
    kmdw_model_fw_info_t* fw_info_ptr;
    kmdw_model_fw_info_ext_t* fw_info_ext_ptr = NULL;

    if (0 != s_model_data.n_ddr_addr_model_end) {
        ret = s_model_data.n_ddr_addr_model_end;
        goto FUNC_OUT;
    }

    fw_info_ptr = kmdw_model_get_fw_info(trust_ddr_data);
    fw_info_ext_ptr = _get_fw_info_ext_by_fw_info(fw_info_ptr);

    if (fw_info_ext_ptr) {
        ret = fw_info_ext_ptr->model_dram_addr_end;
    }

FUNC_OUT:

    dbg_msg("%s = 0x%x\n", __FUNCTION__, ret);

    return ret;
}

int kmdw_model_get_input_tensor_num(uint32_t model_type)
{
    int model_idx = 0;
    struct kdp_model_s *p_model_info = NULL;

    model_idx = _get_model_info_array_index_by_model_type(model_type);
    if (model_idx >= 0) {
        p_model_info = kmdw_model_get_model_info(model_idx);
    } else {
        err_msg("[%s] invalid model id %d\n", __FUNCTION__, model_type);
        return 0;
    }

    bool is_flatbuffer = (SETUP_LEGACY_MAGIC_NUM != *((uint32_t *)p_model_info->setup_mem_addr));

    if (!is_flatbuffer) {
        /******************************************************************
         * legacy setup.bin model
         ******************************************************************/
        return ((CNN_Header *)p_model_info->setup_mem_addr)->input_num;
    } else {
        /******************************************************************
         * flatbuffer setup.bin model
         ******************************************************************/
        Kneron_INFContent_table_t root = Kneron_INFContent_as_root((void *)p_model_info->setup_mem_addr);

        if (NULL == root) {
            err_msg("invalid model setup buffer\n");
            return 0;
        }

        Kneron_Tensor_vec_t tensor_vec = Kneron_INFContent_inputs(root);
        return Kneron_Tensor_vec_len(tensor_vec);
    }
}

int kmdw_model_get_input_tensor_info(uint32_t model_type, uint32_t tensor_idx, kmdw_model_tensor_descriptor_t *tensor_info)
{
    int ret = 1;
    int model_idx = 0;
    uint32_t p_setup_bin = 0;
    uint32_t npu_input_mem_address = 0;
    bool is_flatbuffer = true;
    uint32_t node_num = 0;
    uint32_t setup_buff_offset = sizeof(CNN_Header);
    uint32_t setup_buff_size = 0;
    uint32_t npu_input_mem_size = 0;
    uint32_t npu_input_mem_end_address = 0;
    uint32_t npu_input_node_end_address = 0;

    if (NULL == tensor_info) {
        err_msg("[%s] NULL tensor_info pointer\n", __FUNCTION__);
        ret = 0;
        goto FUNC_OUT;
    }

    model_idx = _get_model_info_array_index_by_model_type(model_type);
    if (model_idx >= 0) {
        struct kdp_model_s *p_model_info = kmdw_model_get_model_info(model_idx);
        p_setup_bin = p_model_info->setup_mem_addr;
        setup_buff_size = p_model_info->setup_mem_len;
        npu_input_mem_address = p_model_info->input_mem_addr;
        npu_input_mem_size = p_model_info->input_mem_len;
    } else {
        err_msg("[%s] invalid model id %d\n", __FUNCTION__, model_type);
        ret = 0;
        goto FUNC_OUT;
    }

    is_flatbuffer = (SETUP_LEGACY_MAGIC_NUM != *((uint32_t *)p_setup_bin));

    if (!is_flatbuffer) {
        /******************************************************************
         * legacy setup.bin model
         ******************************************************************/
        node_num = ((CNN_Header *)p_setup_bin)->input_num;

        if (tensor_idx >= node_num) {
            err_msg("[%s] tensor index out of range %d\n", __FUNCTION__, tensor_idx);
            ret = 0;
            goto FUNC_OUT;
        }

        NetInput_Node *target_input_node = NULL;
        NetInput_Node *input_node = NULL;
        while ((setup_buff_offset < setup_buff_size) && (NULL == target_input_node)) {
            uintptr_t node_buff = (uintptr_t)p_setup_bin + setup_buff_offset;
            uint32_t node_id = *(uint32_t *)node_buff;
            uint32_t node_offset = 0;

            switch (node_id) {
            case NODE_TYPE_IN:
                // NPU IN Signal NODE
                dbg_msg("current node is an NPU IN Signal NODE\n");
                node_offset = sizeof(In_Node);
                break;
            case NODE_TYPE_CPU:
                // CPU NODE
                dbg_msg("current node is a CPU NODE\n");
                node_offset = sizeof(CPU_Node) - (2 * sizeof(Data_Node*));
                break;
            case NODE_TYPE_OUTPUT:
                // OUTPUT NODE
                dbg_msg("current node is a output NODE\n");
                node_offset = sizeof(Out_Node) - (sizeof(Super_Node*));
            case NODE_TYPE_INPUT:
                // NPU INPUT NODE
                dbg_msg("current node is an network input NODE\n");
                input_node = (NetInput_Node *)node_buff;
                node_offset = sizeof(NetInput_Node);

                if (input_node->input_index == tensor_idx)
                    target_input_node = input_node;
                break;
            case NODE_TYPE_DATA:
                // NPU DATA NODE
                dbg_msg("current node is an network data NODE\n");
                node_offset = sizeof(Data_Node) - sizeof(Super_Node*);
                break;
            case NODE_TYPE_SUPER:
                // NPU SUPER NODE
                dbg_msg("current node is an network super NODE\n");
                node_offset = sizeof(Super_Node);
                break;
            default:
                // Unknown NODE
                err_msg("[%s] unknown node type: %d\n", __FUNCTION__, node_id);
                ret = 0;
                goto FUNC_OUT;
            }

            setup_buff_offset += node_offset;
        }

        if (NULL == target_input_node) {
            err_msg("[%s] can not find target index node %d\n", __FUNCTION__, tensor_idx);
            ret = 0;
            goto FUNC_OUT;
        }

        tensor_info->index =            target_input_node->input_index;
        tensor_info->shape_npu_len =    4;
        tensor_info->shape_npu[0] =     1;
        tensor_info->shape_npu[1] =     target_input_node->input_channel;
        tensor_info->shape_npu[2] =     target_input_node->input_row;
        tensor_info->shape_npu[3] =     target_input_node->input_col;
        tensor_info->data_layout =      target_input_node->input_format;
        tensor_info->scale =            1.0;
        tensor_info->radix =            target_input_node->input_radix;
        tensor_info->address =          target_input_node->input_start + npu_input_mem_address;
        tensor_info->size =             target_input_node->input_size;
    } else {
        /******************************************************************
         * flatbuffer setup.bin model
         ******************************************************************/
        Kneron_INFContent_table_t root = Kneron_INFContent_as_root((void *)p_setup_bin);

        if (NULL == root) {
            err_msg("[%s] invalid model setup buffer\n", __FUNCTION__);
            ret = 0;
            goto FUNC_OUT;
        }

        Kneron_Tensor_vec_t tensor_vec =    Kneron_INFContent_inputs(root);
        node_num =                          Kneron_Tensor_vec_len(tensor_vec);

        if (tensor_idx >= node_num) {
            err_msg("[%s] tensor index out of range %d\n", __FUNCTION__, tensor_idx);
            ret = 0;
            goto FUNC_OUT;
        }

        tensor_info->index =                    tensor_idx;

        Kneron_Tensor_table_t tensor =          Kneron_Tensor_vec_at(tensor_vec, tensor_idx);
        tensor_info->data_layout =              Kneron_Tensor_format(tensor);

        flatbuffers_int32_vec_t shape_npu =     Kneron_Tensor_shape(tensor);
        tensor_info->shape_npu_len =            flatbuffers_int32_vec_len(shape_npu);

        if (4 != tensor_info->shape_npu_len) {
            err_msg("[%s] invalid input tensor shape %d\n", tensor_info->shape_npu_len);
            ret = 0;
            goto FUNC_OUT;
        }

        memcpy(tensor_info->shape_npu, shape_npu, tensor_info->shape_npu_len * flatbuffers_int32__size());

        Kneron_QuantizationParameters_table_t quantization_parameters_flatbuffer =  Kneron_Tensor_quantization(tensor);
        Kneron_FxpInfo_vec_t fxp_info_vec_flatbuffer =                              Kneron_QuantizationParameters_fxp_info(quantization_parameters_flatbuffer);
        uint32_t quantized_fixed_point_descriptor_num =                             Kneron_FxpInfo_vec_len(fxp_info_vec_flatbuffer);
        if (1 != quantized_fixed_point_descriptor_num) {
            err_msg("[%s] invalid number of input tensor quantization parameters %d\n", quantized_fixed_point_descriptor_num);
            ret = 0;
            goto FUNC_OUT;
        }

        Kneron_FxpInfo_table_t fxp_info_flatbuffer =    Kneron_FxpInfo_vec_at(fxp_info_vec_flatbuffer, 0);
        tensor_info->radix =                            Kneron_FxpInfo_radix(fxp_info_flatbuffer);
        tensor_info->scale =                            Kneron_FxpInfo_scale(fxp_info_flatbuffer);

        Kneron_Header_table_t header_flatbuffer =       Kneron_INFContent_header(root);
        switch (Kneron_Header_addressing_mode(header_flatbuffer))
        {
        case Kneron_AddressingMode_Absolute:
            npu_input_mem_address = 0;
            break;
        case Kneron_AddressingMode_Relative:
        default:
            break;
        } 

        Kneron_Buffer_table_t buffer_flatbuffer =       Kneron_Tensor_buffer(tensor);
        tensor_info->address =                          Kneron_Buffer_entry(buffer_flatbuffer) + npu_input_mem_address;
        tensor_info->size =                             Kneron_Buffer_len(buffer_flatbuffer);
    }

    npu_input_mem_end_address = npu_input_mem_address + npu_input_mem_size;
    npu_input_node_end_address = tensor_info->address + tensor_info->size;
    if (npu_input_mem_address > tensor_info->address) {
        err_msg("[%s] invalid input node address 0x%x (NPU input memory address 0x%x)\n", tensor_info->address, npu_input_mem_address);
        ret = 0;
        goto FUNC_OUT;
    }

    if ((npu_input_mem_end_address) < (npu_input_node_end_address)) {
        err_msg("[%s] invalid input node end address 0x%x (NPU input memory end address 0x%x)\n", npu_input_node_end_address, npu_input_mem_end_address);
        ret = 0;
        goto FUNC_OUT;
    }

FUNC_OUT:
    return ret;
}

int kmdw_model_get_output_tensor_num(uint32_t model_type)
{
    int model_idx = 0;
    struct kdp_model_s *p_model_info = NULL;

    model_idx = _get_model_info_array_index_by_model_type(model_type);
    if (model_idx >= 0) {
        p_model_info = kmdw_model_get_model_info(model_idx);
    } else {
        err_msg("[%s] invalid model id %d\n", __FUNCTION__, model_type);
        return 0;
    }

    bool is_flatbuffer = (SETUP_LEGACY_MAGIC_NUM != *((uint32_t *)p_model_info->setup_mem_addr));

    if (!is_flatbuffer) {
        /******************************************************************
         * legacy setup.bin model
         ******************************************************************/
        return ((CNN_Header *)p_model_info->setup_mem_addr)->output_num;
    } else {
        /******************************************************************
         * flatbuffer setup.bin model
         ******************************************************************/
        Kneron_INFContent_table_t root = Kneron_INFContent_as_root((void *)p_model_info->setup_mem_addr);

        if (NULL == root) {
            err_msg("[%s] invalid model setup buffer\n", __FUNCTION__);
            return 0;
        }

        Kneron_Tensor_vec_t tensor_vec = Kneron_INFContent_outputs(root);
        return Kneron_Tensor_vec_len(tensor_vec);
    }
}

int kmdw_model_get_output_tensor_info(uint32_t model_type, uint32_t tensor_idx, kmdw_model_tensor_descriptor_t *tensor_info)
{
    int ret = 1;
    int model_idx = 0;
    uint32_t p_setup_bin = 0;
    uint32_t npu_output_mem_address = 0;
    bool is_flatbuffer = true;
    uint32_t node_num = 0;
    uint32_t setup_buff_offset = sizeof(CNN_Header);
    uint32_t setup_buff_size = 0;
    uint32_t npu_output_mem_size = 0;
    uint32_t npu_output_mem_end_address = 0;
    uint32_t npu_output_node_end_address = 0;

    if (NULL == tensor_info) {
        err_msg("[%s] NULL tensor_info pointer\n", __FUNCTION__);
        ret = 0;
        goto FUNC_OUT;
    }

    model_idx = _get_model_info_array_index_by_model_type(model_type);
    if (model_idx >= 0) {
        struct kdp_model_s *p_model_info = kmdw_model_get_model_info(model_idx);
        p_setup_bin = p_model_info->setup_mem_addr;
        setup_buff_size = p_model_info->setup_mem_len;
        npu_output_mem_address = p_model_info->output_mem_addr;
        npu_output_mem_size = p_model_info->output_mem_len;
    } else {
        err_msg("[%s] invalid model id %d\n", __FUNCTION__, model_type);
        ret = 0;
        goto FUNC_OUT;
    }

    is_flatbuffer = (SETUP_LEGACY_MAGIC_NUM != *((uint32_t *)p_setup_bin));

    if (!is_flatbuffer) {
        /******************************************************************
         * legacy setup.bin model
         ******************************************************************/
        node_num = ((CNN_Header *)p_setup_bin)->output_num;

        if (tensor_idx >= node_num) {
            err_msg("[%s] tensor index out of range %d\n", __FUNCTION__, tensor_idx);
            ret = 0;
            goto FUNC_OUT;
        }

        Super_Node *target_super_node = NULL;
        Out_Node *target_output_node = NULL;
        Out_Node *output_node = NULL;
        uint32_t data_size, grid_w, grid_h, grid_c, grid_c_aligned, grid_w_aligned;
        uint32_t grid_w_round_num, grid_c_round_num;
        while ((setup_buff_offset < setup_buff_size) && (NULL == target_output_node) && (NULL == target_super_node)) {
            uintptr_t node_buff = (uintptr_t)p_setup_bin + setup_buff_offset;
            uint32_t node_id = *(uint32_t *)node_buff;
            uint32_t node_offset = 0;

            switch (node_id) {
            case NODE_TYPE_IN:
                // NPU IN Signal NODE
                dbg_msg("current node is an NPU IN Signal NODE\n");
                node_offset = sizeof(In_Node);
                break;
            case NODE_TYPE_CPU:
                // CPU NODE
                dbg_msg("current node is a CPU NODE\n");
                node_offset = sizeof(CPU_Node) - (2 * sizeof(Data_Node*));
                break;
            case NODE_TYPE_OUTPUT:
                // OUTPUT NODE
                dbg_msg("current node is a output NODE\n");
                output_node = (Out_Node *)node_buff;
                node_offset = sizeof(Out_Node) - (sizeof(Super_Node*));

                if (output_node->output_index == tensor_idx) {
                    target_output_node = output_node;
                    target_super_node = (Super_Node *)(node_buff + node_offset);

                    if (NODE_TYPE_SUPER != *(uint32_t *)target_super_node)
                        target_super_node = NULL;
                }
                break;
            case NODE_TYPE_INPUT:
                // NPU INPUT NODE
                dbg_msg("current node is an network input NODE\n");
                node_offset = sizeof(NetInput_Node);
                break;
            case NODE_TYPE_DATA:
                // NPU DATA NODE
                dbg_msg("current node is an network data NODE\n");
                node_offset = sizeof(Data_Node) - sizeof(Super_Node*);
                break;
            case NODE_TYPE_SUPER:
                // NPU SUPER NODE
                dbg_msg("current node is an network super NODE\n");
                node_offset = sizeof(Super_Node);
                break;
            default:
                // Unknown NODE
                err_msg("[%s] unknown node type: %d\n", __FUNCTION__, node_id);
                ret = 0;
                goto FUNC_OUT;
            }

            setup_buff_offset += node_offset;
        }

        if ((NULL == target_output_node) || (NULL == target_super_node)) {
            err_msg("[%s] can not find target index node %d\n", __FUNCTION__, tensor_idx);
            ret = 0;
            goto FUNC_OUT;
        }

        tensor_info->index =            target_output_node->output_index;
        tensor_info->shape_npu_len =    4;
        tensor_info->shape_npu[0] =     1;
        tensor_info->shape_npu[1] =     target_output_node->ch_length;
        tensor_info->shape_npu[2] =     target_output_node->row_length;
        tensor_info->shape_npu[3] =     target_output_node->col_length;
        tensor_info->data_layout =      target_output_node->data_format;
        tensor_info->scale =            *(float *)&(target_output_node->output_scale);
        tensor_info->radix =            target_output_node->output_radix;
        tensor_info->address =          target_super_node->addr + npu_output_mem_address;

        switch (target_output_node->data_format)
        {
        case FMT_1W16C8B:
            grid_w_round_num = 1;
            grid_c_round_num = 16;
            break;
        case FMT_16W1C8B:
            grid_w_round_num = 16;
            grid_c_round_num = 1;
            break;
        case FMT_8W1C16B:
            grid_w_round_num = 8;
            grid_c_round_num = 1;
            break;
        default:
            err_msg("Error: Invalid output npu data layout %u\n", target_output_node->data_format);
            ret = 0;
            goto FUNC_OUT;
        }

        grid_w =            target_output_node->col_length;
        grid_h =            target_output_node->row_length;
        grid_c =            target_output_node->ch_length;
        data_size =         (FMT_8W1C16B == target_output_node->data_format) ? sizeof(int16_t) : sizeof(int8_t);

        grid_w_aligned =    _round_up_with_num(grid_w, grid_w_round_num);
        grid_c_aligned =    _round_up_with_num(grid_c, grid_c_round_num);

        tensor_info->size = grid_h * grid_w_aligned * grid_c_aligned * data_size;
    } else {
        /******************************************************************
         * flatbuffer setup.bin model
         ******************************************************************/
        Kneron_INFContent_table_t root = Kneron_INFContent_as_root((void *)p_setup_bin);

        if (NULL == root) {
            err_msg("[%s] invalid model setup buffer\n", __FUNCTION__);
            ret = 0;
            goto FUNC_OUT;
        }

        Kneron_Tensor_vec_t tensor_vec =    Kneron_INFContent_outputs(root);
        node_num =                          Kneron_Tensor_vec_len(tensor_vec);

        if (tensor_idx >= node_num) {
            err_msg("[%s] tensor index out of range %d\n", __FUNCTION__, tensor_idx);
            ret = 0;
            goto FUNC_OUT;
        }

        tensor_info->index =                    tensor_idx;

        Kneron_Tensor_table_t tensor =          Kneron_Tensor_vec_at(tensor_vec, tensor_idx);
        tensor_info->data_layout =              Kneron_Tensor_format(tensor);

        flatbuffers_int32_vec_t shape_npu =     Kneron_Tensor_shape(tensor);
        tensor_info->shape_npu_len =            flatbuffers_int32_vec_len(shape_npu);

        if (4 != tensor_info->shape_npu_len) {
            err_msg("[%s] invalid output tensor shape %d\n", tensor_info->shape_npu_len);
            ret = 0;
            goto FUNC_OUT;
        }

        memcpy(tensor_info->shape_npu, shape_npu, tensor_info->shape_npu_len * flatbuffers_int32__size());

        Kneron_QuantizationParameters_table_t quantization_parameters_flatbuffer =  Kneron_Tensor_quantization(tensor);
        Kneron_FxpInfo_vec_t fxp_info_vec_flatbuffer =                              Kneron_QuantizationParameters_fxp_info(quantization_parameters_flatbuffer);
        uint32_t quantized_fixed_point_descriptor_num =                             Kneron_FxpInfo_vec_len(fxp_info_vec_flatbuffer);
        if (1 != quantized_fixed_point_descriptor_num) {
            err_msg("[%s] invalid number of output tensor quantization parameters %d\n", quantized_fixed_point_descriptor_num);
            ret = 0;
            goto FUNC_OUT;
        }

        Kneron_FxpInfo_table_t fxp_info_flatbuffer =    Kneron_FxpInfo_vec_at(fxp_info_vec_flatbuffer, 0);
        tensor_info->radix =                            Kneron_FxpInfo_radix(fxp_info_flatbuffer);
        tensor_info->scale =                            Kneron_FxpInfo_scale(fxp_info_flatbuffer);

        Kneron_Header_table_t header_flatbuffer =       Kneron_INFContent_header(root);
        switch (Kneron_Header_addressing_mode(header_flatbuffer))
        {
        case Kneron_AddressingMode_Absolute:
            npu_output_mem_address = 0;
            break;
        case Kneron_AddressingMode_Relative:
        default:
            break;
        }

        Kneron_Buffer_table_t buffer_flatbuffer =       Kneron_Tensor_buffer(tensor);
        tensor_info->address =                          Kneron_Buffer_entry(buffer_flatbuffer) + npu_output_mem_address;
        tensor_info->size =                             Kneron_Buffer_len(buffer_flatbuffer);
    }

    npu_output_mem_end_address = npu_output_mem_address + npu_output_mem_size;
    npu_output_node_end_address = tensor_info->address + tensor_info->size;
    if (npu_output_mem_address > tensor_info->address) {
        err_msg("[%s] invalid output node address 0x%x (NPU output memory address 0x%x)\n", tensor_info->address, npu_output_mem_address);
        ret = 0;
        goto FUNC_OUT;
    }

    if ((npu_output_mem_end_address) < (npu_output_node_end_address)) {
        err_msg("[%s] invalid output node end address 0x%x (NPU output memory end address 0x%x)\n", npu_output_node_end_address, npu_output_mem_end_address);
        ret = 0;
        goto FUNC_OUT;
    }

FUNC_OUT:
    return ret;
}

#if DEBUG

void kmdw_model_dump_model_info(void)
{
    //kdp_model_mgr_t *mgr = kdp_modelmgr();
    struct kdp_model_s *p_modelInfo = 0;
    uint8_t i;

    dbg_msg("Model info Count = %d\n", s_model_data.n_model_count);

    for (i = 0 ; i < s_model_data.n_model_count ; i++) {
        p_modelInfo = &(kmdw_model_data.p_model_info[i]);
        dbg_msg("Model(%2d) model_type(%3d)/version(%5d):\n",
                (i+1),
                p_modelInfo->model_type, p_modelInfo->model_version);

        dbg_msg("input[%x](sz:%d) -> cmd[%x](sz:%d),weight[%x](sz:%d),setup[%x](sz:%d),buf[%x](sz:%d) -> out[%x](sz:%d)\n",
                (i+1),
                p_modelInfo->input_mem_addr, p_modelInfo->input_mem_len,
                p_modelInfo->cmd_mem_addr,   p_modelInfo->cmd_mem_len,
                p_modelInfo->weight_mem_addr,p_modelInfo->weight_mem_len,
                p_modelInfo->setup_mem_addr, p_modelInfo->setup_mem_len,
                p_modelInfo->buf_addr,       p_modelInfo->buf_len,
                p_modelInfo->output_mem_addr,p_modelInfo->output_mem_len);
    }

    return;
}

#endif // DEBUG
